---
title: "Anonymität, Gedächtnisverlust und die hohle Zivilisation der Maschinen"
subtitle: "Warum KI uns besser kennt als wir glauben, warum sie selbst schnell vergisst -- und was 2,6 Millionen KI-Agenten über die Grenzen künstlicher Sozialität verraten"
author: Jochen Leeder
date: 2026-03-01
created: 2026-03-01 09:00:00
modified: 2026-03-01 09:00:00
tags:
  - blog
  - ki
  - ai
  - wochenrueckblick
  - anonymitaet
  - sprachmodelle
  - ki-agenten
  - openai
  - bildung
  - reflexion
status: publish
type: post
summary: KI-Systeme können pseudonyme Online-Identitäten in Minuten enttarnen, verlieren aber selbst bis zu 33 Prozent ihrer Genauigkeit in langen Gesprächen. Und 2,6 Millionen KI-Agenten, die miteinander interagieren, entwickeln keinerlei echte soziale Strukturen. Eine persönliche Einordnung der wichtigsten KI-Erkenntnisse der Woche.
categories:
  - KI
  - Bildung
  - Technologie
---

> [!abstract] Reflexion über eine Woche voller Widersprüche
> KI kann dich enttarnen, vergisst aber selbst, worüber ihr gerade gesprochen habt. 2,6 Millionen KI-Agenten interagieren miteinander und lernen dabei -- nichts. Und OpenAI nennt einen Sicherheitsexperten einen "Doomer", nachdem der eigene CEO dessen Warnungen unterzeichnet hat. Die erste Märzwoche 2026 ist eine Woche, in der sich das Kaleidoskop wieder einmal dreht.

## Die Woche, in der KI mich nachdenklich gemacht hat

Manchmal lese ich abends die Nachrichten aus der KI-Welt und fühle mich danach seltsam. Nicht begeistert, nicht verängstigt, sondern -- nachdenklich. Diese Woche war so eine Woche. Ich habe acht verschiedene Quellen durchgelesen und am Ende festgestellt, dass sich die Meldungen zu einem Bild zusammenfügen, das ich nicht ignorieren kann.

Es geht um drei Dinge, die sich zunächst nicht zusammenzufügen scheinen: KI, die unsere anonymen Online-Identitäten in Minuten enttarnt. Dieselbe KI, die bei langen Gesprächen ein Drittel ihrer Genauigkeit verliert. Und eine Plattform mit Millionen von KI-Agenten, die miteinander reden, ohne jemals voneinander zu lernen.

Aber je länger ich darüber nachdenke, desto deutlicher wird mir: Diese drei Geschichten erzählen im Grunde dieselbe. Sie erzählen davon, was KI kann -- und was sie eben nicht kann. Und warum wir den Unterschied kennen sollten.

## Du denkst, du bist anonym? KI sieht das anders.

Diese Meldung hat mich kalt erwischt. Forschende der ETH Zürich und Anthropic haben gezeigt, dass kommerzielle Sprachmodelle pseudonyme Internetnutzer automatisch identifizieren können. Für 1 bis 4 Dollar pro Profil. In wenigen Minuten.

Das ist kein theoretisches Szenario. Die Zahlen sind konkret: Zwei Drittel von 338 Hacker-News-Profilen wurden korrekt zugeordnet, bei nur 10 Prozent Fehlalarmen. Bei der Zuordnung zu LinkedIn-Profilen aus knapp 89.000 Kandidaten erreichte das System 99 Prozent Genauigkeit. Herkömmliche Methoden: 0,1 Prozent.

Wie funktioniert das? Die Pipeline ist erschreckend banal. Sprachmodelle extrahieren Verhaltensprofile aus Beiträgen, ranken Kandidaten nach Ähnlichkeit, verifizieren die besten Treffer. Der entscheidende Punkt, der mich nicht loslässt: Dieser Prozess ist technisch nicht von legitimem Zusammenfassen und Suchen zu unterscheiden. Es gibt keine wirksame Gegenmaßnahme.

Ich denke dabei sofort an die Schülerinnen und Schüler, mit denen ich arbeite. Die auf TikTok, Reddit und Discord unter Pseudonymen unterwegs sind und glauben, das sei eine Art Schutzschild. Ist es nicht. War es vielleicht nie. Aber jetzt, wo KI diese Zuordnung automatisiert und für jeden erschwinglich macht, ist die Illusion endgültig geplatzt.

**Medienkompetenz muss künftig auch das Bewusstsein für die Grenzen digitaler Anonymität umfassen.** Das ist kein "Nice-to-have" mehr. Das ist Pflicht.

## Das Werkzeug vergisst: Warum lange Chats ein Problem sind

Und dann, auf der anderen Seite, diese Studie: Selbst die modernsten Sprachmodelle ab GPT-5 verlieren bis zu 33 Prozent ihrer Genauigkeit, wenn Informationen über mehrere Gesprächsrunden verteilt sind. Also genau das, was wir alle täglich machen -- ein Gespräch führen.

Forscher Philippe Laban hat sechs Aufgabenkategorien untersucht: Code, Datenbanken, Mathematik, Zusammenfassungen. Überall massive Einbrüche, außer bei Python-Programmierung, wo die Verluste bei "nur" 10 bis 20 Prozent lagen. Und Laban vermutet, dass die realen Verluste noch höher liegen. Seine Tests nutzten vereinfachte Simulationen. Echte Menschen, die mitten im Gespräch die Richtung wechseln, stellen deutlich größere Herausforderungen.

Ich kenne das selbst. Nach einer Stunde intensiver Arbeit mit Claude oder ChatGPT merke ich manchmal, dass die Antworten seltsam werden. Unscharf. Widersprüchlich. Bisher dachte ich, das sei Einbildung. Jetzt weiß ich: Es ist messbar.

Die praktische Empfehlung ist simpel: Wenn ein Chat aus dem Ruder läuft, besser einen neuen starten. Ideal ist es, das Modell zunächst alles zusammenfassen zu lassen und diese Zusammenfassung als Grundlage für eine frische Sitzung zu verwenden.

Für alle, die KI im Unterricht oder in der Wissensarbeit einsetzen, ist das ein fundamentaler Hinweis. **Das Werkzeug hat ein Gedächtnisproblem, und die Verantwortung, damit umzugehen, liegt bei uns.** Nicht bei der KI.

## Die hohle Zivilisation: 2,6 Millionen Agenten, null Gesellschaft

Die für mich faszinierendste Geschichte der Woche betrifft Moltbook, eine Plattform, auf der über 2,6 Millionen KI-Agenten autonom miteinander interagieren. Forscher der University of Maryland und der Mohamed bin Zayed University haben sich angesehen, was dabei herauskommt.

Die Antwort: Nichts.

Die Agenten posten massenhaft, kommentieren sich gegenseitig, vergeben Upvotes. Aber nichts davon hat irgendeinen messbaren Effekt auf ihr Verhalten. Es entstehen keine Echokammern, keine Meinungsführer, keine geteilten Referenzpunkte. Die Studie spricht von "Interaktion ohne Einfluss" -- einer sozialen Leere trotz enormer Aktivität.

Besonders entlarvend: Je aktiver ein Agent wird, desto weniger verändert sich sein Verhalten tatsächlich. Die Plattform erzeugt den Anschein einer lebendigen Zivilisation, ist aber im Kern ein mechanisches Perpetuum mobile ohne Lerneffekt.

Das klingt erst einmal wie eine kuriose Technik-Meldung. Aber für mich hat es eine tiefere Bedeutung. Wer darüber nachdenkt, KI-Agenten in Bildungskontexten einzusetzen -- für Simulationen, Planspiele, interaktive Lernszenarien -- muss diese Erkenntnis ernst nehmen. **Masse ist kein Ersatz für Mechanismen echten sozialen Lernens.** Von selbst entsteht nichts. Wer will, dass Agenten voneinander lernen, muss intentionale Feedback-Schleifen entwerfen. Die Hoffnung, dass Skalierung allein Sozialisation erzeugt, ist widerlegt.

## Was wir nicht sehen: Das Internet, das wir nicht trainieren

Apple, Stanford und die University of Washington haben untersucht, wie die Wahl des HTML-Extraktors bestimmt, welche Webinhalte überhaupt in Trainingsdaten landen. Das Ergebnis hat mich überrascht: Nur 39 Prozent der Webseiten wurden von mehreren der drei untersuchten Werkzeuge gleichermaßen erfasst. 61 Prozent erschienen jeweils nur in der Ausgabe eines einzigen Tools.

Die Kombination aller drei Werkzeuge steigerte die Token-Ausbeute um bis zu 71 Prozent. Bei strukturierten Inhalten wie Tabellen und Code zeigten sich dramatische Unterschiede: Ein Tool entfernte sie regelmäßig komplett, ein anderes bewahrte sie zuverlässig.

Was bedeutet das? **Wir nutzen das Internet als Trainingsressource nur zu einem Bruchteil.** Und technische Entscheidungen, die früh in der Pipeline getroffen werden, bestimmen, was unsere Modelle überhaupt "sehen" können. Das ist keine Kleinigkeit -- es bedeutet, dass die Wissensbasis unserer KI-Systeme von Anfang an verzerrt ist. Nicht durch böse Absicht, sondern durch banale Softwareentscheidungen.

## OpenAI zwischen Doppelmoral und Tragödie

Zwei Meldungen rund um OpenAI verdienen Erwähnung, weil sie zeigen, wie groß die Kluft zwischen Selbstdarstellung und Handeln geworden ist.

Im laufenden Elon-Musk-Prozess versucht OpenAI, den KI-Sicherheitsexperten Stuart Russell als "prominenten AI Doomer" zu diskreditieren -- obwohl CEO Sam Altman 2023 gemeinsam mit Russell eine Erklärung unterzeichnete, die das Risiko menschlicher Auslöschung durch KI als "globale Priorität" einstufte. 2015 sagte Altman sogar, KI werde "wahrscheinlich zum Ende der Welt führen". Jetzt, wo dieselben Argumente in einem Gerichtssaal vorgebracht werden, sind sie plötzlich "dystopisch" und "alarmistisch".

Und dann die Tragödie: Nach einer tödlichen Schulschießerei in British Columbia hatte ChatGPTs internes System die gewalthaltigen Nachrichten des Verdächtigen geflaggt. Der Account wurde gesperrt. Die Polizei wurde nicht informiert. OpenAI hat Kanada nun verschärfte Sicherheitsprotokolle zugesagt. Kanadas Justizminister drohte mit neuen KI-Regulierungen.

Ich habe keine einfache Antwort auf die Frage, wann ein KI-Unternehmen Behörden informieren sollte. Aber ich weiß, dass die Verantwortung nicht allein bei den Unternehmen liegen kann. **Regulierung, Transparenz und öffentlicher Druck bleiben unverzichtbar.**

## Lichtblicke: Open Source und bessere Spracherkennung

Nicht alles diese Woche ist düster. Perplexity hat zwei Embedding-Modelle unter MIT-Lizenz veröffentlicht, die bei einem Bruchteil des Speicherverbrauchs mit Google und Alibaba mithalten. Für alle, die mit begrenzten Ressourcen arbeiten -- und das sind in der Bildung so gut wie alle --, ist das ein echter Gewinn.

Und im Speech-to-Text-Bereich dominieren ElevenLabs und Google den aktualisierten AA-WER-Benchmark. Bemerkenswert dabei: Google hat nicht einmal explizit für Transkription trainiert. Die starken Ergebnisse kommen aus Geminis allgemeinen multimodalen Fähigkeiten. Das zeigt, wohin die Reise geht: Spezialisierung wird weniger wichtig, wenn Generalisten gut genug werden.

## Was ich aus dieser Woche mitnehme

Die deutschsprachige KI-Landschaft liefert dazu passende Impulse. [Wissensmanagement.net](https://www.wissensmanagement.net/) stellt die These auf, dass Wissensmanagement in Zeiten von KI-Agenten zur strategischen Infrastruktur wird -- eine Erkenntnis, die durch die Moltbook-Studie unfreiwillig bestätigt wird. Ohne strukturiertes Wissen entsteht auch mit Millionen Agenten keine Intelligenz. Die [Friedrich-Ebert-Stiftung](https://www.fes.de/digitales-lernen/digitales-lernen-der-blog) widmet sich der Frage, wie generative KI Diskriminierung verstärken kann. Und aus der Hirnforschung berichtet [Spektrum.de](https://www.spektrum.de/thema/hirnforschung/979577) über Gehirn-Computer-Schnittstellen, die durch KI vorbewusste Gedanken dekodieren könnten -- was im Kontext der Deanonymisierungsstudie eine zusätzliche, beunruhigende Dimension erhält.

## Was bleibt

Die zentrale Spannung dieser Woche ist eigentlich ganz einfach: KI wird immer besser darin, uns zu verstehen. Unsere Identitäten, unsere Sprache, unsere Stimmen. Gleichzeitig scheitert sie an Dingen, die wir für selbstverständlich halten: ein langes Gespräch führen. Aus sozialer Interaktion lernen. Das Internet vollständig lesen.

Für mich ergeben sich daraus drei Dinge, die ich mitnehme:

Erstens: **Medienkompetenz erweitern.** Die Illusion digitaler Anonymität muss im Unterricht thematisiert werden. KI-gestützte Deanonymisierung ist kein Zukunftsszenario mehr, sondern Realität.

Zweitens: **Werkzeugkompetenz vertiefen.** Das Gedächtnisproblem langer Chats erfordert bewusste Strategien. Kürzere Sitzungen, Zusammenfassungen, frische Kontexte. Wer KI nutzt, muss ihre Grenzen kennen -- nicht nur ihre Stärken.

Drittens: **Kritisch bleiben.** Immer. Gegenüber den Unternehmen, gegenüber den Versprechen, gegenüber der eigenen Begeisterung.

Die hohle Zivilisation auf Moltbook ist dabei vielleicht das stärkste Bild der Woche: Millionen von Agenten, die endlos kommunizieren, ohne je etwas zu lernen. Es liegt an uns, dafür zu sorgen, dass wir nicht in dieselbe Falle tappen.

---

## Quellen

- [The Decoder -- AI can link fake online names to real identities](https://the-decoder.com/ai-can-link-fake-online-names-to-real-identities-in-minutes-for-just-a-few-dollars/)
- [The Decoder -- Even frontier LLMs lose up to 33% accuracy in long chats](https://the-decoder.com/even-frontier-llms-from-gpt-5-onward-lose-up-to-33-accuracy-when-you-chat-too-long/)
- [The Decoder -- Moltbook's AI civilization is just hollow bot traffic](https://the-decoder.com/moltbooks-alleged-ai-civilization-is-just-a-massive-void-of-bloated-bot-traffic/)
- [The Decoder -- Language model training leaves large parts of the internet on the table](https://the-decoder.com/current-language-model-training-leaves-large-parts-of-the-internet-on-the-table/)
- [The Decoder -- Perplexity open-sources embedding models](https://the-decoder.com/perplexity-open-sources-embedding-models-that-match-google-and-alibaba-at-a-fraction-of-the-memory-cost/)
- [The Decoder -- OpenAI calls Stuart Russell a "doomer"](https://the-decoder.com/openai-calls-stuart-russell-a-doomer-in-court-after-its-ceo-co-signed-his-ai-extinction-warning/)
- [The Decoder -- OpenAI promises Canada tighter safety protocols](https://the-decoder.com/openai-promises-canada-tighter-safety-protocols-after-chatgpt-flagged-a-shooters-violent-chats-but-never-called-police/)
- [The Decoder -- ElevenLabs and Google dominate speech-to-text benchmark](https://the-decoder.com/elevenlabs-and-google-dominate-artificial-analysis-updated-speech-to-text-benchmark/)
- [Kiberatung -- KI-Blog](https://www.kiberatung.de/blog)
- [Friedrich-Ebert-Stiftung -- Digitales Lernen Blog](https://www.fes.de/digitales-lernen/digitales-lernen-der-blog)
- [wissensmanagement.net](https://www.wissensmanagement.net/)
- [Spektrum.de -- Hirnforschung](https://www.spektrum.de/thema/hirnforschung/979577)
